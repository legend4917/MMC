\chapter{支持向量机SVM}
SVM是Vapnik\upcite{vladimir1995nature}在1995年提出的一种非常有效的算法，由于其卓绝的分类性能，很快成为机器学习的主流技术。以统计学习理论为基础，SVM提出了间隔 (Margin) 的概念，不但提高了分类精确率，而且保证其训练得到的分类器具有较好的泛化能力。并且在此基础上引入核技巧，在有效解决维数灾难问题的同时，也成为更具实用性的非线性分类模型。本章将重点介绍SVM模型的相关基础知识，分别从SVM的主问题和对偶问题出发，分析其间隔最大化学习策略和核技巧的应用。
\section{SVM模型}
\subsection{硬间隔最大化分类器}
SVM是定义在特征空间上间隔最大的分类器。考虑图\ref{fig:svm}所示的二维二类线性可分的情况：
\begin{figure}[!htbp]
\begin{pspicture}(20,6)
\psline[linewidth=1pt](4,3)(9,1)
\psline[linewidth=1pt](4.345,3.862)(9.345,1.862)
\psline[linewidth=1pt](4.69,4.724)(9.69,2.724)
\psline[linewidth=1pt]{<->}(9,1)(9.69,2.724)
\rput(11.5,1.8){Margin$=\displaystyle\frac{2}{\sqrt{\mathbf{w}^T*\mathbf{w}}}$}
\rput(4,4){$L$}
\rput(3.7,3){$L_1$}
\rput(4.3,4.8){$L_2$}
\psdot[dotscale=1.7](4,2)
\psdot[dotscale=2.7,dotstyle=o](5,2.6)
\psdot[dotscale=1.7](5,2.6)
\psdot[dotscale=1.7](5.8,1.7)
\psdot[dotscale=2.7,dotstyle=o](6.6,1.96)
\psdot[dotscale=1.7](6.6,1.96)
\psdot[dotscale=1.7](7,0.7)
\psdot[dotscale=1.7](8,0.99)
\psdot[dotscale=1.7,dotstyle=o](6.6,4.96)
\psdot[dotscale=1.7,dotstyle=o](5.6,4.96)
\psdot[dotscale=2.7,dotstyle=o](6.4,4.04)
\psdot[dotscale=1.7,dotstyle=o](6.4,4.04)
\psdot[dotscale=1.7,dotstyle=o](7.6,4.06)
\psdot[dotscale=1.7,dotstyle=o](8.3,4.01)
\psdot[dotscale=2.7,dotstyle=o](8.8,3.08)
\psdot[dotscale=1.7,dotstyle=o](8.8,3.08)
\psdot[dotscale=1.7,dotstyle=o](9.6,3.46)
\end{pspicture}
\caption{二维二类分类问题}
\label{fig:svm}
\end{figure}

图中分别使用实心点和空心点表示两类训练样本。其中$L$是将两类样本正确分类的直线，也即分类线，$L_1$和$L_2$ 分别为过各类样本集合中离分类线最近的样本点且平行于分类线$L$的直线，将两类样本之间的间隔定义为直线$L_1$和$L_2$之间的距离。如果分类线$L$不但能够将两类样本完全正确的分开，并且使得两类样本之间的间隔最大，那么就认为分类线$L$是最优分类线。最优分类线不但能最小化经验风险，而且还能使泛化性能最优,从而使得结构风险最小。若将其推广到高维空间，那么最优分类线就变成最优分类面。

考虑两类分类问题，给定训练集$T=\{(\mathbf{x}_i,y_i)\}^m_{i=1}$，其中$\mathbf{x}_i\in X=\mathbf{\mathbb{R}}^n$，$y_i \in \{-1,+1\}$是训练样本所对应的类别标记。则其相应的分类决策函数为$f(x)=\mathrm{sign}(\mathbf{w}^T\mathbf{x}+b)$，分离超平面为：
\begin{align} % requires amsmath; align* for no eq. number
   \mathbf{w}^T\mathbf{x}+b=0
\end{align}

其中，$\mathbf{w}$是权值向量，$b$是偏置。定义超平面$(\mathbf{w},b)$关于训练数据集$T$的函数间隔：
\begin{align} % requires amsmath; align* for no eq. number
   \hat{\gamma}=\min_{1,\cdots,m}y_i(\mathbf{w}^T\mathbf{x}_i+b)
\end{align}

进一步规范化分离超平面的法向量$\mathbf{w}$，也即令$\|\mathbf{w}\|=1$，这时便得到超平面$(\mathbf{w},b)$关于训练数据集$T$的几何间隔：
\begin{align} % requires amsmath; align* for no eq. number
   \gamma=\frac{\hat{\gamma}}{\|\mathbf{w}\|}
   \label{equ:margin} 
\end{align}

那么，求解使得几何间隔最大的分离超平面问题就可以公式化的表示为下面的约束最优化问题：
\begin{equation}
\begin{split} % requires amsmath; align* for no eq. number
   \max_{\mathbf{w},b} \quad & \gamma \\
   s.t. \quad & y_i \left( \frac{\mathbf{w}^T}{\|\mathbf{w}\|}\mathbf{x}_i+\frac{b}{\|\mathbf{w}\|} \right) \ge \gamma, \quad i=1,2,\cdots,m
\end{split}
\end{equation}

考虑函数间隔和几何间隔之间的关系式(\ref{equ:margin})，便可得到线性可分支持向量机学习的最优化问题：
\begin{equation}
\begin{split} % requires amsmath; align* for no eq. number
   \max_{\mathbf{w},b} \quad & \frac{1}{2}\|\mathbf{w}\|^2 \\
   s.t. \quad & y_i(\mathbf{w}^T\mathbf{x}_i+b)-1 \ge 0, \quad i=1,2,\cdots,m
\end{split}
\end{equation}

\subsection{软间隔最大化分类器}
在训练数据线性不可分的情况下，上面的线性可分问题的SVM学习方法是不适用的，这就需要对硬间隔最大化进行松弛，得到软间隔最大化。假设训练数据集$T$中存在一些特异点(outlier)，使得训练集$T$是线性不可分的。对其中的每个样本点$(\mathbf{x}_i,y_i)$引进一个松弛变量$\xi_i \ge 0$，使得函数间隔加上松弛变量后大于等于1。同时对每个松弛变量$\xi_i$都要付出一定的代价，便得到线性不可分的线性支持向量机的学习问题：
\begin{equation}
\begin{split} % requires amsmath; align* for no eq. number
   \max_{\mathbf{w},b} \quad & \frac{1}{2}\|\mathbf{w}\|^2+C\sum^m_{i=1}\xi_i \\
   s.t. \quad & y_i(\mathbf{w}^T\mathbf{x}_i+b) \ge 1-\xi_i, \quad i=1,2,\cdots,m \\
   & \xi_i \ge 0, \quad  i=1,2,\cdots,m
\end{split}
\end{equation}

其中，$C > 0$是惩罚参数，用来对错误分类的样本进行惩罚。目标函数包含两层含义：几何间隔尽可能的大，同时使错误分类的样本个数尽可能的小。$C$是调和二者的系数。

\section{对偶问题}
针对上面的软间隔SVM学习问题：
\begin{equation}
\begin{split} % requires amsmath; align* for no eq. number
   \max_{\mathbf{w},b} \quad & \frac{1}{2}\|\mathbf{w}\|^2+C\sum^m_{i=1}\xi_i \\
   s.t. \quad & y_i(\mathbf{w}^T\mathbf{x}_i+b) \ge 1-\xi_i, \quad i=1,2,\cdots,m \\
   & \xi_i \ge 0, \quad  i=1,2,\cdots,m
\end{split}
\label{equ:dualPro}
\end{equation}

求解对偶问题 (\ref{equ:dualPro}) 常用的方法是引入拉格朗日函数：
\begin{align} % requires amsmath; align* for no eq. number
   L(\mathbf{w},b,\mathbf{\xi},\mathbf{\alpha},\mathbf{\mu})=\frac{1}{2}\|\mathbf{w}\|^2+C\sum^{m}_{i=1}\xi_i-\sum^{m}_{i=1}\alpha_i(y_i(\mathbf{w}^T\mathbf{x}_i+b)-1+\xi_i)-\sum^m_{i=1}\mu_i\xi_i \label{equ:Lagrange}
\end{align}

其中$\mathbf{\alpha}$和$\mathbf{\mu}$是拉格朗日乘子，且满足$\mathbf{\xi} \ge 0,\mathbf{\mu} \ge 0$。进一步求$L(\mathbf{w},b,\mathbf{\xi},\mathbf{\alpha},\mathbf{\mu})$对$\mathbf{w},b,\mathbf{\xi}$的极小，即：
\begin{align} % requires amsmath; align* for no eq. number
   \nabla_{\mathbf{w}}L(\mathbf{w},b,\mathbf{\xi},\mathbf{\alpha},\mathbf{\mu})=0 \Rightarrow \mathbf{w}=\sum^m_{i=1}\alpha_iy_i\mathbf{x}_i \label{equ:w} \\
   \nabla_{b}L(\mathbf{w},b,\mathbf{\xi},\mathbf{\alpha},\mathbf{\mu})=0 \Rightarrow \sum^m_{i=1}\alpha_iy_i=0 \label{equ:b} \\
   \nabla_{\xi_i}L(\mathbf{w},b,\mathbf{\xi},\mathbf{\alpha},\mathbf{\mu})=0 \Rightarrow C-\alpha_i-\mu_i=0 \label{equ:xi}
\end{align}

将式(\ref{equ:w})$\sim$(\ref{equ:xi})代入式(\ref{equ:Lagrange})中，则可将SVM主问题转化为下面的对偶问题：
\begin{equation}
\begin{split} % requires amsmath; align* for no eq. number
   \min_{\mathbf{\alpha}} \quad & \frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}\alpha_i\alpha_jy_iy_j(\mathbf{x}_i^T\mathbf{x}_j)-\sum^m_{i=1}\alpha_i \\
   s.t. \quad & \sum^m_{i=1}\alpha_iy_i=0 \\
   & 0 \le \alpha_i \le C \quad  i=1,2,\cdots,m
   \label{equ:dual}
\end{split}
\end{equation}

SVM学习问题最终转化为其对偶问题(\ref{equ:dual})进行求解。常用的优化求解算法主要包括：序列最小最优化方法 (SMO)\upcite{cristianini2000introduction}、选块算法\upcite{cristianini2000introduction}和分解算法\upcite{cristianini2000introduction}。

\section{核技巧}
核技巧是一种用线性分类方法求解非线性分类问题的技术，首先使用一个变换将原空间的数据映射到新空间，然后在新空间里用线性分类学习方法从训练数据中学习分类模型\upcite{李航2012统计学习方法}。

在SVM中应用核技巧，其基本想法就是通过一个非线性变换将输入空间 (欧式空间$\mathbb{R}^n$或离散集合) 对应于一个特征空间 (希尔伯特空间$\mathcal{H}$)，使得在输入空间$\mathbb{R}^n$中的超曲面模型对应于特征空间$\mathbb{H}$中的超平面模型 (支持向量机)，这样，分类问题的学习任务通过在特征空间中求解线性SVM就可以完成\upcite{李航2012统计学习方法}。下面首先定义核函数的概念：
\begin{definition}
\emph{设$\mathcal{X}$是输入空间 (欧式空间$\mathbb{R}^n$的子集或离散集合)，又设$\mathcal{H}$为特征空间 (希尔伯特空间)，如果存在一个从$\mathcal{X}$到$\mathcal{H}$的映射：}
\begin{align} % requires amsmath; align* for no eq. number
   \phi(x): \mathcal{X} \to \mathcal{H}
\end{align}

\emph{使得对所有$x,z \in \mathcal{X}$，函数$K(x,z)$满足条件：}
\begin{align} % requires amsmath; align* for no eq. number
   K(x,z) = \phi(x)\cdot \phi(z)
\end{align}

\emph{则称$K(x,z)$为核函数，$\phi(x)$为映射函数，式中$\phi(x)\cdot\phi(z)$为$\phi(x)$与$\phi(z)$的内积\upcite{李航2012统计学习方法}。}
\end{definition}

核技巧的想法是在学习和预测过程中，只显示定义核函数$K(x,z)$，而不是显示定义映射函数$\phi$。观察SVM的对偶学习问题(\ref{equ:dual})的目标函数可以发现$(\mathbf{x}_i^T\mathbf{x}_j)$是两个样本的内积，可以使用核函数$K(\mathbf{x}_i,\mathbf{x}_j)$来代替，便得到非线性SVM的最优化问题：
\begin{equation}
\begin{split} % requires amsmath; align* for no eq. number
   \min_{\mathbf{\alpha}} \quad & \frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}\alpha_i\alpha_jy_iy_jK(\mathbf{x}_i,\mathbf{x}_j)-\sum^m_{i=1}\alpha_i \\
   s.t. \quad & \mathbf{\alpha}^T\mathbf{y}=0 \\
   & 0 \le \alpha_i \le C \quad  i=1,2,\cdots,m
   \label{equ:dual-kernel}
\end{split}
\end{equation}

这等价于将原来的输入空间经过映射函数$\phi$转换到一个新的特征空间，使用特征空间中的内积$\phi(x_i)\cdot\phi(x_j)$来代替输入空间中的内积$(\mathbf{x}_i^T\mathbf{x}_j)$，在训练样本的新的特征空间中学习线性SVM。当映射函数为非线性函数时，学习得到的SVM模型是非线性模型。

核技巧的使用让SVM有效解决了维数灾难问题。此外，根据具体问题的数据样本的分布特点，选择合适的核函数更加有利于向学习问题嵌入其先验知识。

\section{本章小结}
本章主要介绍了 SVM 模型的相关基础知识，分别从SVM的主问题和其对偶问题出发，探讨了模型的构建、约束最优化问题的推导以及核技巧的引入等问题。详细介绍了SVM中的间隔最大化学习策略，以及核技巧的应用方法，为本文后续的研究工作提供了必要的知识背景。鉴于SVM在分类领域所取得的巨大的成功，那么，如果将间隔最大化学习策略和核技巧应用在无监督的聚类学习中，会取得怎样的效果呢{\fangsong ？}受这一灵感的启发，接下来一章开始探讨最大间隔聚类(MMC)算法的原理和模型推导过程。